{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oU3v-cs2hVny"
      },
      "outputs": [],
      "source": [
        "!pip install pandas==2.2.2 numpy==1.24.4 matplotlib==3.10.0 seaborn==0.13.2 gensim==4.3.3 pyLDAvis==3.4.1 d3blocks==1.4.11"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MY_AhWm7hUiw"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import pickle\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "from gensim import models\n",
        "from gensim.models import CoherenceModel\n",
        "from gensim import corpora\n",
        "from gensim.models import LdaModel\n",
        "from scipy.stats import entropy\n",
        "import pyLDAvis.gensim\n",
        "from d3blocks import D3Blocks"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GfBZV84XhP1j"
      },
      "source": [
        "Import preprocessed files"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dj3m7cDaazqo"
      },
      "outputs": [],
      "source": [
        "with open('preprocessed_docs.pkl', 'rb') as f:\n",
        "    preprocessed_docs = pickle.load(f)\n",
        "\n",
        "with open('bow_corpus.pkl', 'rb') as f:\n",
        "    bow_corpus = pickle.load(f)\n",
        "\n",
        "with open('dictionary.dict', 'rb') as f:\n",
        "    dictionary = pickle.load(f)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_O0oJxPshecL"
      },
      "source": [
        "Plot mean tokens per abstract"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bLHmODt1haDO"
      },
      "outputs": [],
      "source": [
        "num_tokens_per_abstract = [len(doc) for doc in preprocessed_docs]\n",
        "plt.figure(figsize=(10, 6))\n",
        "sns.violinplot(y=num_tokens_per_abstract, color='skyblue', inner='point')\n",
        "plt.title('Distribution of Mean Number of Tokens per Abstract')\n",
        "plt.ylabel('Number of Tokens')\n",
        "dpi_value = 600\n",
        "plt.savefig('violin_plot.png', dpi=dpi_value)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LLhh4t2vhsm9"
      },
      "source": [
        "Run coherence model to calculate optimal number of topics for LDA topic modelling"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KHkwzltAh5Dk"
      },
      "outputs": [],
      "source": [
        "# Choose a range of potential number of topics\n",
        "min_topics = 5\n",
        "max_topics = 50\n",
        "step_size = 5\n",
        "topics_range = range(min_topics, max_topics, step_size)\n",
        "\n",
        "# Run LDA for each number of topics and calculate coherence scores\n",
        "coherence_scores = []\n",
        "for num_topics in topics_range:\n",
        "    lda_model = LdaModel(bow_corpus, num_topics=num_topics, id2word=dictionary, passes=1)\n",
        "    coherence_model_lda = CoherenceModel(model=lda_model, texts=preprocessed_docs, dictionary=dictionary, coherence='c_v')\n",
        "    coherence_score = coherence_model_lda.get_coherence()\n",
        "    coherence_scores.append(coherence_score)\n",
        "\n",
        "# Print the number of topics with the highest coherence score\n",
        "optimal_num_topics = topics_range[coherence_scores.index(max(coherence_scores))]\n",
        "print(\"Optimal number of topics:\", optimal_num_topics)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4edRBUZEiS6u"
      },
      "source": [
        "LDA topic model training and visualization with pyLDAvis"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WS7TLMv7iMlA"
      },
      "outputs": [],
      "source": [
        "# Train the LDA model\n",
        "lda_model = LdaModel(bow_corpus, num_topics=30, id2word=dictionary, passes=20)\n",
        "for idx, topic in lda_model.print_topics():\n",
        "    print(f'Topic: {idx}')\n",
        "    print(f'Top words: {topic}')\n",
        "    print()\n",
        "\n",
        "# Save the model\n",
        "with open('lda_model.pkl', 'wb') as f:\n",
        "    pickle.dump(lda_model, f)\n",
        "\n",
        "# Visualizing the LDA model using pyLDAvis and save as html\n",
        "pyLDAvis.enable_notebook()\n",
        "vis = pyLDAvis.gensim.prepare(lda_model, bow_corpus, dictionary)\n",
        "pyLDAvis.display(vis)\n",
        "pyLDAvis.save_html(vis, 'lda_visualization.html')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XZNGCNO7jT1U"
      },
      "source": [
        "TF-IDF model training and top TF-IDF words per topic"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WcrP4Df1icZs"
      },
      "outputs": [],
      "source": [
        "# Train TF-IDF model\n",
        "tfidf_model = models.TfidfModel(bow_corpus)\n",
        "tfidf_corpus = tfidf_model[bow_corpus]\n",
        "\n",
        "# Get the topic distribution for each document\n",
        "doc_topics = [lda_model.get_document_topics(doc) for doc in bow_corpus]\n",
        "\n",
        "# Initialize TF-IDF scores for each topic\n",
        "topic_tfidf_scores = np.zeros((lda_model.num_topics, len(dictionary)))\n",
        "\n",
        "# Calculate TF-IDF scores for each word in each topic\n",
        "for doc, doc_topic in zip(tfidf_corpus, doc_topics):\n",
        "    for topic, score in doc_topic:\n",
        "        for word_id, word_score in doc:\n",
        "            topic_tfidf_scores[topic, word_id] += score * word_score\n",
        "\n",
        "# Rank and print top 20 words by TF-IDF scores within each topic\n",
        "N = 20\n",
        "top_words_per_topic = []\n",
        "for topic_idx in range(lda_model.num_topics):\n",
        "    top_word_indices = np.argsort(topic_tfidf_scores[topic_idx])[::-1][:N]\n",
        "    top_words = [(dictionary[word_id], topic_tfidf_scores[topic_idx][word_id]) for word_id in top_word_indices]\n",
        "    top_words_per_topic.append(top_words)\n",
        "for topic_idx, top_words in enumerate(top_words_per_topic):\n",
        "    print(f'Topic {topic_idx}:')\n",
        "    for word, score in top_words:\n",
        "        print(f'{word}: {score}')\n",
        "    print()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "W_1OWqePjwj5"
      },
      "source": [
        "Evolution of topic proportions, calculate 5-year moving average to smooth the data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vr9XXFMhscX2"
      },
      "outputs": [],
      "source": [
        "# Load the preprocessed_docs.csv that contains the year of publication data in 'Year' column and convert to numeric\n",
        "file_name = 'preprocessed_docs.csv'\n",
        "df = pd.read_csv(file_name, low_memory=False)\n",
        "df['Year'] = pd.to_numeric(df['Year'], errors='coerce')\n",
        "df.reset_index(drop=True, inplace=True)\n",
        "\n",
        "# Replace NaN years with 1 as a placeholder\n",
        "df['Year'] = df['Year'].fillna(1).astype(int)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wjbGKyXykv0N"
      },
      "outputs": [],
      "source": [
        "num_docs = len(preprocessed_docs)\n",
        "all_years = sorted(df['Year'].unique())\n",
        "\n",
        "# Calculate proportion of each topic in each year iterating over each document\n",
        "topic_proportions = pd.DataFrame(columns=range(lda_model.num_topics), index=all_years)\n",
        "for index, doc_topics in enumerate(lda_model.get_document_topics(bow_corpus[:num_docs], minimum_probability=0)):\n",
        "    year = int(df.iloc[index]['Year'])\n",
        "    for topic, prob in doc_topics:\n",
        "        topic_proportions.at[year, topic] = topic_proportions.at[year, topic] + prob if not pd.isnull(topic_proportions.at[year, topic]) else prob\n",
        "\n",
        "# Normalize to get proportions\n",
        "topic_proportions = topic_proportions.div(topic_proportions.sum(axis=1), axis=0)\n",
        "\n",
        "# Filter only 1975-2023 for plotting\n",
        "topic_proportions = topic_proportions.loc[1975:2023]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wIbNdwU7-sB_"
      },
      "outputs": [],
      "source": [
        "# Save topic_proportions df to a CSV file\n",
        "topic_proportions.to_csv('topic_proportions.csv', index_label='Year')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hPlMqrRujwCc"
      },
      "outputs": [],
      "source": [
        "# Define original topic names in the model's order\n",
        "original_topic_names = [\n",
        "    \"Cell signaling\", \"Development\", \"CNS diseases\", \"Cardiovascular\",\n",
        "    \"Age-related decline\", \"Risk factors\", \"Cell biology\", \"Gender\",\n",
        "    \"Muscle\", \"Oxidative stress\", \"Bone\", \"Therapeutics\",\n",
        "    \"Metabolism\", \"Neural tissue\", \"Clinics\", \"Healthcare\",\n",
        "    \"General terms\", \"Brain structure\", \"Psychosocial\", \"Rodent studies\",\n",
        "    \"Cancer\", \"Physical activity\", \"Demography\", \"Liver and kidney\",\n",
        "    \"Genetics\", \"Analytics\", \"Cognition\", \"Physics\",\n",
        "    \"Skin\", \"Clinical tests\"\n",
        "]\n",
        "\n",
        "# Define the desired order of topic names\n",
        "desired_order = [\n",
        "    \"General terms\", \"Healthcare\", \"Cell biology\", \"Genetics\", \"Analytics\", \"Cell signaling\",\n",
        "    \"Demography\", \"Clinical tests\", \"Age-related decline\", \"Rodent studies\", \"Clinics\",\n",
        "    \"Psychosocial\", \"Oxidative stress\", \"Physics\", \"Therapeutics\", \"Risk factors\",\n",
        "    \"Development\", \"Cognition\", \"CNS diseases\", \"Skin\", \"Neural tissue\", \"Brain structure\",\n",
        "    \"Cancer\", \"Metabolism\", \"Physical activity\", \"Cardiovascular\", \"Gender\", \"Muscle\",\n",
        "    \"Bone\", \"Liver and kidney\"\n",
        "]\n",
        "\n",
        "# Create a mapping from original topic indices to the desired order and plot a 6x5 grid\n",
        "topic_mapping = {name: i for i, name in enumerate(original_topic_names)}\n",
        "desired_indices = [topic_mapping[name] for name in desired_order]\n",
        "fig, axes = plt.subplots(6, 5, figsize=(26, 24))\n",
        "axes = axes.flatten()\n",
        "for i, topic_index in enumerate(desired_indices):\n",
        "    moving_avg = topic_proportions[topic_index].rolling(window=5, min_periods=1).mean()\n",
        "    axes[i].plot(topic_proportions.index, moving_avg, label=f'Topic {topic_index}', color='navy')\n",
        "    axes[i].set_title(desired_order[i])\n",
        "    axes[i].set_xlabel('Year')\n",
        "    axes[i].set_ylabel('Proportion')\n",
        "    axes[i].grid(False)\n",
        "    axes[i].set_xticks([1975, 1985, 1995, 2005, 2015, 2023])\n",
        "plt.tight_layout()\n",
        "plt.savefig('topic_evolution.pdf')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zy_oqELv-8K5"
      },
      "source": [
        "Shannon entropy evolution (10-year moving average)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JP4MaIoB-naw"
      },
      "outputs": [],
      "source": [
        "shannon_entropy_per_document = []\n",
        "years = []\n",
        "\n",
        "for index, doc_topics in enumerate(lda_model.get_document_topics(bow_corpus[:num_docs], minimum_probability=0)):\n",
        "    year = df.iloc[index]['Year']\n",
        "    years.append(year)\n",
        "    topic_probabilities = [prob for _, prob in doc_topics]\n",
        "    shannon_entropy = entropy(topic_probabilities, base=2)  # Using base 2 for binary entropy\n",
        "    shannon_entropy_per_document.append(shannon_entropy)\n",
        "\n",
        "entropy_data = pd.DataFrame({'Year': years, 'Shannon_Entropy': shannon_entropy_per_document})\n",
        "entropy_by_year = entropy_data.groupby('Year')['Shannon_Entropy'].mean()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wSewA6QHBRAU"
      },
      "outputs": [],
      "source": [
        "# Plotting\n",
        "\n",
        "rolling_entropy = entropy_by_year.rolling(window=10, center=True).mean()\n",
        "rolling_entropy_filtered = rolling_entropy[(rolling_entropy.index >= 1975) & (rolling_entropy.index <= 2023)]\n",
        "\n",
        "plt.figure(figsize=(10, 6))\n",
        "plt.plot(rolling_entropy_filtered.index, rolling_entropy_filtered.values, linestyle='-', color='#006d77', linewidth=2, label='10-Year Moving Average')\n",
        "plt.xlabel('Year')\n",
        "plt.ylabel('Mean Shannon Entropy')\n",
        "plt.title('Mean Shannon Entropy of Topic Distributions Over the Years')\n",
        "plt.xticks(np.arange(1975, 2024, 5))\n",
        "plt.legend()\n",
        "plt.savefig('mean_shannon_entropy_over_years.pdf', dpi=600)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OWSSGdv5D-MX"
      },
      "source": [
        "Topic co-occurence matrix generation and chord diagram plotting"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iAlYWIudDrB5"
      },
      "outputs": [],
      "source": [
        "# Calculate topic distribution for each document and generate a co-occurrence matrix\n",
        "topic_distributions = [lda_model.get_document_topics(doc) for doc in bow_corpus]\n",
        "num_topics = lda_model.num_topics\n",
        "co_occurrence_matrix = np.zeros((num_topics, num_topics))\n",
        "\n",
        "for dist in topic_distributions:\n",
        "    # Sort topics by probability and take the top 5\n",
        "    top_topics = sorted(dist, key=lambda x: x[1], reverse=True)[:5]\n",
        "    top_topic_indices = [topic[0] for topic in top_topics]\n",
        "    for i in range(len(top_topic_indices)):\n",
        "        for j in range(i + 1, len(top_topic_indices)):\n",
        "            co_occurrence_matrix[top_topic_indices[i], top_topic_indices[j]] += 1\n",
        "            co_occurrence_matrix[top_topic_indices[j], top_topic_indices[i]] += 1\n",
        "\n",
        "# Normalize the co-occurrence matrix\n",
        "co_occurrence_matrix /= len(topic_distributions)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pFJIeZxjEOVc"
      },
      "outputs": [],
      "source": [
        "# Plotting using D3Blocks\n",
        "\n",
        "topic_names = [\n",
        "    \"Cell signaling\",\n",
        "    \"Development\",\n",
        "    \"CNS diseases\",\n",
        "    \"Cardiovascular\",\n",
        "    \"Age-related decline\",\n",
        "    \"Risk factors\",\n",
        "    \"Cell biology\",\n",
        "    \"Gender\",\n",
        "    \"Muscle\",\n",
        "    \"Oxidative stress\",\n",
        "    \"Bone\",\n",
        "    \"Therapeutics\",\n",
        "    \"Metabolism\",\n",
        "    \"Neural tissue\",\n",
        "    \"Clinics\",\n",
        "    \"Healthcare\",\n",
        "    \"General terms\",\n",
        "    \"Brain structure\",\n",
        "    \"Psychosocial\",\n",
        "    \"Rodent studies\",\n",
        "    \"Cancer\",\n",
        "    \"Physical activity\",\n",
        "    \"Demography\",\n",
        "    \"Liver and kidney\",\n",
        "    \"Genetics\",\n",
        "    \"Analytics\",\n",
        "    \"Cognition\",\n",
        "    \"Physics\",\n",
        "    \"Skin\",\n",
        "    \"Clinical tests\"\n",
        "]\n",
        "\n",
        "labels = [topic_names[i] if i < len(topic_names) else f'Topic {i}' for i in range(num_topics)]\n",
        "links = []\n",
        "threshold = 0.033  # Adjust the threshold as needed\n",
        "for i in range(co_occurrence_matrix.shape[0]):\n",
        "    for j in range(i + 1, co_occurrence_matrix.shape[1]):  # Ensure i < j to avoid duplicates\n",
        "        if co_occurrence_matrix[i, j] > threshold:\n",
        "            links.append((labels[i], labels[j], co_occurrence_matrix[i, j]))\n",
        "\n",
        "\n",
        "df = pd.DataFrame(links, columns=['source', 'target', 'weight'])\n",
        "d3 = D3Blocks(chart='Chord', frame=False)\n",
        "d3.chord(df, arrowhead=0, title='Topic Co-occurrence Chord Diagram', cmap='tab20', figsize=(800, 800), filepath='/content/chord_diagram.html')"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
