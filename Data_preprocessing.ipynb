{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "Load the dataset as a csv containing an 'Abstract' and 'Year' column, tokenize the text filtering out abstracts with less than 30 or more than 500 words. Generate and save preprocessed_docs, bag of words (bow_corpus) and dictionary."
      ],
      "metadata": {
        "id": "pjkWykShYk2l"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9g9hAl7BYbhv"
      },
      "outputs": [],
      "source": [
        "file_name = '/content/drive/My Drive/cleaned_dataset.csv'\n",
        "df = pd.read_csv(file_name, low_memory=False)\n",
        "\n",
        "# Preprocess text data\n",
        "def preprocess_text(text):\n",
        "    tokens = preprocess_string(text)\n",
        "    if 30 <= len(tokens) <= 500:\n",
        "        return tokens\n",
        "    else:\n",
        "        return None\n",
        "\n",
        "df['Processed'] = df['Abstract'].apply(preprocess_text)\n",
        "valid_indices = df['Processed'].dropna().index\n",
        "df_cleaned = df.loc[valid_indices].reset_index(drop=True)\n",
        "\n",
        "preprocessed_docs = df_cleaned['Processed'].tolist()\n",
        "years = df_cleaned['Year'].tolist()\n",
        "original_abstracts = df_cleaned['Abstract'].tolist()\n",
        "\n",
        "# Filter out tokens that appear in less than 5000 documents or more than 30% of the documents to generate a dictionary\n",
        "dictionary = corpora.Dictionary(preprocessed_docs)\n",
        "dictionary.filter_extremes(no_below=5000, no_above=0.3)\n",
        "\n",
        "# Convert the documents to a bag-of-words representation\n",
        "bow_corpus = [dictionary.doc2bow(doc) for doc in preprocessed_docs]\n",
        "\n",
        "# Save the data\n",
        "df_cleaned.to_csv('/content/drive/My Drive/cleaned_dataset_with_preprocessed_docs.csv', index=False)\n",
        "\n",
        "with open('preprocessed_docs.pkl', 'wb') as f:\n",
        "    pickle.dump(preprocessed_docs, f)\n",
        "\n",
        "with open('bow_corpus.pkl', 'wb') as f:\n",
        "    pickle.dump(bow_corpus, f)\n",
        "\n",
        "with open('dictionary.pkl', 'wb') as f:\n",
        "    pickle.dump(dictionary, f)"
      ]
    }
  ]
}